from openai import OpenAI
from dotenv import load_dotenv
import requests
import json

load_dotenv()

client = OpenAI()

def get_weather(city: str) -> str:
    print("ðŸ”¨ Tool Called: get_weather, City: ", city)
    
    url = f"https://wttr.in/{city}?format=%C+%t"
    response = requests.get(url)
    
    if response.status_code == 200:
        return f"The current weather of {city} is {response.text}"
    return "Something went wrong"

user_query = input("ðŸ‘¨: ")

# 1. Define a list of callable tools for the model
tools = [
    {
        "type": "function",
        "name": "get_weather",
        "description": "Takes a city name as an input and returns the current weather for the city",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {
                    "type": "string",
                    "description": "The location of the place where we want the weather.",
                }
            },
            "required": ["city"],
        },
    },
]

# Create a running input list we will add to over time
input_list = [
    { "role": "user", "content": user_query }
]

# 2. Prompt the model with tools defined
response = client.responses.create(
    model="gpt-4o-mini",
    tools=tools,
    input=input_list,
)

input_list += response.output

for item in response.output:
    if item.type == "function_call":
        if item.name == "get_weather":
            # 3. Execute the function logic for get_horoscope
            get_weather = get_weather(json.loads(item.arguments))
            
            # 4. Provide function call results to the model
            input_list.append({
                "type": "function_call_output",
                "call_id": item.call_id,
                "output": json.dumps({
                  "get_weather": get_weather
                })
            })

response = client.responses.create(
    model="gpt-4o-mini",
    instructions="Respond only with a get_weather generated by a tool.",
    tools=tools,
    input=input_list,
)

# 5. The model should be able to give a response!
response.model_dump_json(indent=2)
print("\n" + response.output_text)


    
    




